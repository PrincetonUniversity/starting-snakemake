import os
import hashlib
from urllib.parse import urlparse
# Helper function to get safe filename from URL
def get_safe_name(url):
    """Create a safe filename from URL without dots and special chars."""
    name = url.replace('https://', '').replace('http://', '')
    name = name.replace('.', '_').replace('/', '_')
    return name.strip('_')

URLS = [
    "https://snakemake.readthedocs.io/en/stable/",
    "https://www.princeton.edu/"
]
    
# Generate list of safe filenames
URL_NAMES = [get_safe_name(url) for url in URLS]

# Rules
rule all:
    input:
        "results/combined_frequencies.png",
        expand("results/plots/{url_name}.png", url_name=URL_NAMES)

rule download_html:
    output:
        "data/{url_name}.html"
    params:
        url=lambda wildcards: [url for url in URLS 
                              if get_safe_name(url) == wildcards.url_name][0]
    shell:
        """
        mkdir -p data
        curl -L '{params.url}' -o {output}
        """

rule count_words:
    input:
        "data/{url_name}.html"
    output:
        "results/counts/{url_name}.csv"
    shell:
        """
        mkdir -p results/counts
        cat {input} 2>/dev/null | \
        sed 's/<[^>]*>//g' | \
        tr '[:upper:]' '[:lower:]' | \
        tr -cs '[:alpha:]' '\n' | \
        grep -v "^$" | \
        sort | \
        uniq -c | \
        sort -nr | \
        sed 's/^ *//g' | \
        awk '{{print $2 "," $1}}' > {output}
        """

rule plot_frequencies:
    input:
        "results/counts/{url_name}.csv"
    output:
        "results/plots/{url_name}.png"
    shell:
        """
        mkdir -p results/plots
        python scripts/plot_frequencies.py {input} {output}
        """

rule combine_results:
    input:
        counts=expand("results/counts/{url_name}.csv", url_name=URL_NAMES)
    output:
        "results/combined_frequencies.png"
    shell:
        """
        mkdir -p results
        python scripts/combine_counts.py results/counts {output}
        """